The impact of cross-lingual priming on neural machine translation (NMT) models:

1. **‚ÄúCross-lingual Transferring of Pre-trained Contextualized Language Models‚Äù**
    
    - **Abstract**: This paper proposes a novel cross-lingual model transferring framework for pre-trained contextualized language models (PrLMs). The framework, called TRELM, leverages intermediate structures and cross-lingual language modeling objectives to transfer knowledge from one language to another. Experiments show that TRELM significantly outperforms language models trained from scratch with limited data, both in performance and efficiency.
    - [**Link**:](https://arxiv.org/pdf/2107.12627.pdf) [1](https://arxiv.org/pdf/2107.12627.pdf)
2. **‚ÄúEffective Cross-lingual Transfer of Neural Machine Translation Models‚Äù**
    
    - **Abstract**: This paper explores effective techniques for transferring pre-trained NMT models to new, unrelated languages without shared vocabularies. It improves plain NMT transfer by up to +5.1% BLEU in low-resource translation tasks, outperforming multilingual joint training.
    - [**Link**:](https://arxiv.org/pdf/2107.12627.pdf) [2](https://aclanthology.org/P19-1120/)
3. **‚ÄúEvaluating the Cross-Lingual Effectiveness of Massively Multilingual Neural Machine Translation‚Äù**
    
    - **Abstract**: This study investigates the cross-lingual effectiveness of massively multilingual NMT systems. These systems can translate over 100 languages to and from English within a single model. The improved translation performance on low-resource languages suggests potential cross-lingual transfer capability.
    - [**Link**:](https://arxiv.org/pdf/2107.12627.pdf) [3](https://arxiv.org/abs/1909.00437)
4. **‚ÄúEffective Cross-lingual Transfer of Neural Machine Translation Models‚Äù**
    
    - **Abstract**: This paper provides techniques to transfer a pre-trained NMT model to a new language without shared vocabularies. It outperforms multilingual joint training and offers insights into NMT transfer.
    - [**Link**:](https://arxiv.org/pdf/2107.12627.pdf) [4](https://arxiv.org/abs/1905.05475)

Feel free to explore these papers for deeper insights into cross-lingual priming‚Äôs impact on NMT models! üåêüìö