{
	"nodes":[
		{"id":"de66641a1699df70","type":"group","x":-1680,"y":-440,"width":800,"height":1120,"label":"Untitled group"},
		{"id":"5cce015051e3d32b","type":"group","x":-560,"y":1340,"width":880,"height":500,"label":"Attention Mechanisms"},
		{"id":"a1388cb5b74a2fd8","type":"group","x":-560,"y":-80,"width":880,"height":400,"label":"Introduction to Neural Machine Translation"},
		{"id":"e5c70b71aa5c9e4d","type":"group","x":-560,"y":400,"width":880,"height":400,"label":"Deep Learning for NMT"},
		{"id":"1868ac1fc8ac7659","type":"group","x":-560,"y":880,"width":880,"height":400,"label":"Sequence-to-sequence Model"},
		{"id":"effe0f28eb591bb8","type":"group","x":-560,"y":1920,"width":880,"height":400,"label":"Encoder-Decoder Architecture"},
		{"id":"0cbb3d6868c25a25","type":"group","x":-560,"y":2880,"width":880,"height":400,"color":"#f8f7f7","label":"Advanced Topic in NMT"},
		{"id":"b35489335b95dc0f","type":"group","x":-560,"y":3880,"width":880,"height":400,"label":"Evaluation of NMT"},
		{"id":"5247c801c0766804","type":"group","x":-560,"y":4360,"width":880,"height":400,"label":"Current Research and Future Direction"},
		{"id":"ede73fc3bad77fff","type":"group","x":-560,"y":2400,"width":880,"height":400,"label":"Transformer Architecture"},
		{"id":"24676f39fd7a7b1e","type":"text","text":"- Evaluation Metrics: BLEU, METEOR, TER\n- Issues with Evaluation Metrics\n- Human Evaluation of NMT","x":-540,"y":3900,"width":280,"height":140},
		{"id":"238c608007f1da89","type":"text","text":"- Dealing with Out-of-Vocabulary Words\n- Handling Long Sequences\n- Multilingual NMT\n- Zero-Shot Translation","x":-515,"y":2920,"width":355,"height":160},
		{"id":"62297f4b6679cb81","type":"file","file":"2️⃣ Collections/Code-switching/Code-switching concept.canvas","x":-560,"y":3360,"width":880,"height":440},
		{"id":"68f0cdc92cf802d3","type":"text","text":"- Latest Research Trends in NMT\n- Challenges and Open Problems\n- Future of NMT","x":-540,"y":4380,"width":320,"height":120},
		{"id":"90518e100bebbf81","type":"file","file":"2️⃣ Collections/NLP in CS/Transformer.md","x":-120,"y":2440,"width":400,"height":280},
		{"id":"0eea808a0adb03c1","type":"text","text":"- Introduction to Transformer Architecture\n- Understanding Self-Attention Mechanism\n- Encoder and Decoder in Transformer\n- Positional Encoding in Transformer\n- Training and Evaluation of Transformer Models\n- Case Study: “Attention is All You Need”\n- Challenges and Future Directions in Transformer Models","x":-515,"y":2440,"width":355,"height":280},
		{"id":"71f1eac18bbdff5f","type":"file","file":"2️⃣ Collections/NLP in CS/Encoder-Decoder Architecture.md","x":-120,"y":1980,"width":400,"height":280},
		{"id":"4ac5ed7de91c62fa","type":"text","text":"- Understanding the Encoder-Decoder Structure\n- Variations and Improvements\n- Practical Implementation","x":-515,"y":1980,"width":355,"height":140},
		{"id":"55d5d70dd8b2b800","type":"file","file":"2️⃣ Collections/NLP in CS/Attention mechanism.md","x":-120,"y":1380,"width":400,"height":400},
		{"id":"00d330a15619dad8","type":"text","text":"- Need for Attention in NMT\n- Types of Attention: Global and Local\n- Implementing Attention in Seq2Seq Models","x":-515,"y":1380,"width":355,"height":140},
		{"id":"867a95bebdb33435","type":"text","text":"- Understanding Seq2Seq Models\n- Applications in NMT\n- Training and Evaluation of Seq2Seq Models","x":-515,"y":920,"width":395,"height":120},
		{"id":"7ca49f1f17003633","type":"text","text":"- Introduction to Deep Learning\n- Feedforward Neural Networks\n- Convolutional Neural Networks\n- Recurrent Neural Networks\n- Transformer Networks","x":-515,"y":440,"width":295,"height":200},
		{"id":"1ff47c2041de874d","type":"file","file":"2️⃣ Collections/NLP in CS/Introduction.md","x":-480,"y":160,"width":692,"height":140},
		{"id":"995d512d48bff3b2","type":"text","text":"- Overview of Machine Translation\n- Evolution from Statistical Machine Translation to Neural Machine Translation\n- Basic concepts in NMT","x":-530,"y":-60,"width":530,"height":140},
		{"id":"56830e01e6139f6b","type":"file","file":"2️⃣ Collections/NLP in CS/Neural Machine Translation.md","x":-320,"y":-280,"width":400,"height":120,"color":"#151414"},
		{"id":"b646cc7ce2ca96c2","type":"file","file":"2️⃣ Collections/NLP in CS/Natural Language Processing Systems.md","x":-680,"y":-640,"width":400,"height":120},
		{"id":"36d2ace788959653","type":"file","file":"2️⃣ Collections/NLP in CS/Handling informal context.md","x":-320,"y":-870,"width":400,"height":140},
		{"id":"7e25e6cb4f4b19ad","type":"file","file":"2️⃣ Collections/NLP in CS/NLP stil in low progress of handling informal contet.md","x":-320,"y":-1230,"width":400,"height":241},
		{"id":"48e0462f2f77e320","type":"file","file":"2️⃣ Collections/NLP in CS/Encoder for CS.md","x":680,"y":1650,"width":400,"height":400},
		{"id":"9cdbb295ca4fbe05","type":"file","file":"2️⃣ Collections/NLP in CS/Monolingual NMT.md","x":-1452,"y":1380,"width":400,"height":400,"color":"1"},
		{"id":"73336c805b6253f7","type":"file","file":"2️⃣ Collections/NLP in CS/Untitled 4.md","x":-1572,"y":140,"width":400,"height":400},
		{"id":"581d0717fe344f1a","type":"file","file":"2️⃣ Collections/NLP in CS/Untitled 2.md","x":-1502,"y":103,"width":400,"height":400},
		{"id":"0cb057fb827c44b7","type":"file","file":"2️⃣ Collections/NLP in CS/Untitled.md","x":-1519,"y":-440,"width":269,"height":473},
		{"id":"7bb30368725a58a9","type":"file","file":"2️⃣ Collections/NLP in CS/Decoder for CS.md","x":680,"y":1180,"width":400,"height":400},
		{"id":"f566ab9c372f6a6f","type":"file","file":"2️⃣ Collections/NLP in CS/Positional Encoder for CS.md","x":680,"y":2400,"width":400,"height":360}
	],
	"edges":[
		{"id":"54dbbbba2f28d7ca","fromNode":"b646cc7ce2ca96c2","fromSide":"bottom","toNode":"56830e01e6139f6b","toSide":"left","label":"branch"},
		{"id":"cee9bf04d1251d17","fromNode":"b646cc7ce2ca96c2","fromSide":"right","toNode":"36d2ace788959653","toSide":"bottom","label":"lag behind"},
		{"id":"7ace8ffe105ce28f","fromNode":"56830e01e6139f6b","fromSide":"top","toNode":"36d2ace788959653","toSide":"bottom"},
		{"id":"8b7247da99778bea","fromNode":"36d2ace788959653","fromSide":"top","toNode":"7e25e6cb4f4b19ad","toSide":"bottom"}
	]
}